{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS CELL.\n",
    "\n",
    "#Please run the following command to install all the required packages for this project.\n",
    "#pip install -r requirements.txt\n",
    "\n",
    "#The following cell mentions all the features with its label which are included in this machine learning project of \n",
    "#detecting phishing website.\n",
    "@attribute having_IP_Address  { -1,1 }   \n",
    "@attribute URL_Length   { 1,0,-1 }\n",
    "@attribute Shortining_Service { 1,-1 }\n",
    "@attribute having_At_Symbol   { 1,-1 }\n",
    "@attribute double_slash_redirecting { -1,1 }\n",
    "@attribute Prefix_Suffix  { -1,1 }\n",
    "@attribute having_Sub_Domain  { -1,0,1 }\n",
    "@attribute SSLfinal_State  { -1,1,0 }\n",
    "@attribute Domain_registeration_length { -1,1 }\n",
    "@attribute Favicon { 1,-1 }\n",
    "@attribute port { 1,-1 }\n",
    "@attribute HTTPS_token { -1,1 }\n",
    "@attribute Request_URL  { 1,-1 }\n",
    "@attribute URL_of_Anchor { -1,0,1 }\n",
    "@attribute Links_in_tags { 1,-1,0 }\n",
    "@attribute SFH  { -1,1,0 }\n",
    "@attribute Submitting_to_email { -1,1 }\n",
    "@attribute Abnormal_URL { -1,1 }\n",
    "@attribute Redirect  { 0,1 }\n",
    "@attribute on_mouseover  { 1,-1 }\n",
    "@attribute RightClick  { 1,-1 }\n",
    "@attribute popUpWidnow  { 1,-1 }\n",
    "@attribute Iframe { 1,-1 }\n",
    "@attribute age_of_domain  { -1,1 }\n",
    "@attribute DNSRecord   { -1,1 }\n",
    "@attribute web_traffic  { -1,0,1 }\n",
    "@attribute Page_Rank { -1,1 }\n",
    "@attribute Google_Index { 1,-1 }\n",
    "@attribute Links_pointing_to_page { 1,0,-1 }\n",
    "@attribute Statistical_report { -1,1 }\n",
    "@attribute Result  { -1,1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS CELL.\n",
    "#This cell includes the code required to extract the particular feature from the URL and to convert it to its corresponding \n",
    "#heuristics.  \n",
    "\n",
    "import ipaddress\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import whois\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dateutil.parser import parse as date_parse\n",
    "\n",
    "# Calculates number of months\n",
    "def diff_month(d1, d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "# Generate data set by extracting the features from the URL\n",
    "def generate_data_set(url):\n",
    "\n",
    "    data_set = []\n",
    "\n",
    "    # Converts the given URL into standard format\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "    # Stores the response of the given URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except:\n",
    "        response = \"\"\n",
    "        soup = -999\n",
    "    # Extracts domain from the given URL\n",
    "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "    if re.match(r\"^www.\",domain):\n",
    "\t       domain = domain.replace(\"www.\",\"\")\n",
    "    # Requests all the information about the domain\n",
    "    whois_response = whois.whois(domain)\n",
    "    rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
    "        \"name\": domain\n",
    "    })\n",
    "    # Extracts global rank of the website\n",
    "    try:\n",
    "        global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
    "    except:\n",
    "        global_rank = -1\n",
    "    # 1.having_IP_Address\n",
    "    try:\n",
    "        ipaddress.ip_address(url)\n",
    "        data_set.append(-1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 2.URL_Length\n",
    "    if len(url) < 54:\n",
    "        data_set.append(1)\n",
    "    elif len(url) >= 54 and len(url) <= 75:\n",
    "        data_set.append(0)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 3.Shortining_Service\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|#from sklearn.svm import SVCscrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "    if match:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 4.having_At_Symbol\n",
    "    if re.findall(\"@\", url):\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 5.double_slash_redirecting\n",
    "    list=[x.start(0) for x in re.finditer('//', url)]\n",
    "    if list[len(list)-1]>6:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 6.Prefix_Suffix\n",
    "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        data_set.append(1)\n",
    "\n",
    "    # 7.having_Sub_Domain\n",
    "    if len(re.findall(\"\\.\", url)) == 1:\n",
    "        data_set.append(1)\n",
    "    elif len(re.findall(\"\\.\", url)) == 2:\n",
    "        data_set.append(0)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 8.SSLfinal_State\n",
    "    try:\n",
    "        if response.text:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 9.Domain_registeration_length\n",
    "    expiration_date = whois_response.expiration_date\n",
    "    registration_length = 0\n",
    "    try:\n",
    "        expiration_date = min(expiration_date)\n",
    "        today = time.strftime('%Y-%m-%d')\n",
    "        today = datetime.strptime(today, '%Y-%m-%d')\n",
    "        registration_length = abs((expiration_date - today).days)\n",
    "\n",
    "        if registration_length / 365 <= 1:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    # 10.Favicon\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            for head in soup.find_all('head'):\n",
    "                for head.link in soup.find_all('link', href=True):\n",
    "                    dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "                    if url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
    "                        data_set.append(1)\n",
    "                        raise StopIteration\n",
    "                    else:\n",
    "                        data_set.append(-1)\n",
    "                        raise StopIteration\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "    #11. port\n",
    "    try:\n",
    "        port = domain.split(\":\")[1]\n",
    "        if port:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    #12. HTTPS_token\n",
    "    if re.findall(r\"^https://\", url):\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    #13. Request_URL\n",
    "    i = 0\n",
    "    success = 0\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        for img in soup.find_all('img', src= True):\n",
    "           dots= [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
    "           if url in img['src'] or domain in img['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for audio in soup.find_all('audio', src= True):\n",
    "           dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "           if url in audio['src'] or domain in audio['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for embed in soup.find_all('embed', src= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',embed['src'])]\n",
    "           if url in embed['src'] or domain in embed['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for iframe in soup.find_all('iframe', src= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',iframe['src'])]\n",
    "           if url in iframe['src'] or domain in iframe['src'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        try:\n",
    "           percentage = success/float(i) * 100\n",
    "           if percentage < 22.0 :\n",
    "              dataset.append(1)\n",
    "           elif((percentage >= 22.0) and (percentage < 61.0)) :\n",
    "              data_set.append(0)\n",
    "           else :\n",
    "              data_set.append(-1)\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "\n",
    "\n",
    "    #14. URL_of_Anchor\n",
    "    percentage = 0\n",
    "    i = 0\n",
    "    unsafe=0\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        for a in soup.find_all('a', href=True):\n",
    "        # 2nd condition was 'JavaScript ::void(0)' but we put JavaScript because the space between javascript and :: might not be\n",
    "                # there in the actual a['href']\n",
    "            if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (url in a['href'] or domain in a['href']):\n",
    "                unsafe = unsafe + 1\n",
    "            i = i + 1\n",
    "\n",
    "\n",
    "        try:\n",
    "            percentage = unsafe / float(i) * 100\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "        if percentage < 31.0:\n",
    "            data_set.append(1)\n",
    "        elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
    "            data_set.append(0)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #15. Links_in_tags\n",
    "    i=0\n",
    "    success =0\n",
    "    if soup == -999:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        for link in soup.find_all('link', href= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',link['href'])]\n",
    "           if url in link['href'] or domain in link['href'] or len(dots)==1:\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "\n",
    "        for script in soup.find_all('script', src= True):\n",
    "           dots=[x.start(0) for x in re.finditer('\\.',script['src'])]\n",
    "           if url in script['src'] or domain in script['src'] or len(dots)==1 :\n",
    "              success = success + 1\n",
    "           i=i+1\n",
    "        try:\n",
    "            percentage = success / float(i) * 100\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "        if percentage < 17.0 :\n",
    "           data_set.append(1)\n",
    "        elif((percentage >= 17.0) and (percentage < 81.0)) :\n",
    "           data_set.append(0)\n",
    "        else :\n",
    "           data_set.append(-1)\n",
    "\n",
    "        #16. SFH\n",
    "        for form in soup.find_all('form', action= True):\n",
    "           if form['action'] ==\"\" or form['action'] == \"about:blank\" :\n",
    "              data_set.append(-1)\n",
    "              break\n",
    "           elif url not in form['action'] and domain not in form['action']:\n",
    "               data_set.append(0)\n",
    "               break\n",
    "           else:\n",
    "                 data_set.append(1)\n",
    "                 break\n",
    "\n",
    "    #17. Submitting_to_email\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #18. Abnormal_URL\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if response.text == \"\":\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #19. Redirect\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if len(response.history) <= 1:\n",
    "            data_set.append(-1)\n",
    "        elif len(response.history) <= 4:\n",
    "            data_set.append(0)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "\n",
    "    #20. on_mouseover\n",
    "    if response == \"\" :\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #21. RightClick\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #22. popUpWidnow\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"alert\\(\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #23. Iframe\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #24. age_of_domain\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        try:\n",
    "            registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
    "            if diff_month(date.today(), date_parse(registration_date)) >= 6:\n",
    "                data_set.append(-1)\n",
    "            else:\n",
    "                data_set.append(1)\n",
    "        except:\n",
    "            data_set.append(1)\n",
    "\n",
    "    #25. DNSRecord\n",
    "    dns = 1\n",
    "    try:\n",
    "        d = whois.whois(domain)\n",
    "    except:\n",
    "        dns=-1\n",
    "    if dns == -1:\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        if registration_length / 365 <= 1:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "\n",
    "    #26. web_traffic\n",
    "    try:\n",
    "        rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        rank= int(rank)\n",
    "        if (rank<100000):\n",
    "            data_set.append(1)\n",
    "        else:\n",
    "            data_set.append(0)\n",
    "    except TypeError:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    #27. Page_Rank\n",
    "    try:\n",
    "        if global_rank > 0 and global_rank < 100000:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        data_set.append(1)\n",
    "\n",
    "    #28. Google_Index\n",
    "    site=search(url, 5)\n",
    "    if site:\n",
    "        data_set.append(1)\n",
    "    else:\n",
    "        data_set.append(-1)\n",
    "\n",
    "    #29. Links_pointing_to_page\n",
    "    if response == \"\":\n",
    "        data_set.append(-1)\n",
    "    else:\n",
    "        number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
    "        if number_of_links == 0:\n",
    "            data_set.append(1)\n",
    "        elif number_of_links <= 2:\n",
    "            data_set.append(0)\n",
    "        else:\n",
    "            data_set.append(-1)\n",
    "\n",
    "    #30. Statistical_report\n",
    "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
    "    try:\n",
    "        ip_address=socket.gethostbyname(domain)\n",
    "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
    "        if url_match:\n",
    "            data_set.append(-1)\n",
    "        elif ip_match:\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            data_set.append(1)\n",
    "    except:\n",
    "        print ('Connection problem. Please check your internet connection!')\n",
    "\n",
    "\n",
    "    print (data_set)\n",
    "    return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, -1, 1, -1, 0, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 0, -1, 1, -1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, -1, 1, -1, 0, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 0, -1, 1, -1, 1, -1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 0, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 1, 1, 0, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1]\n",
      "[1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1]\n",
      "[1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 0, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 0, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#This Cell helps us to create a New Dataset which consist of the required 30 features.\n",
    "#Run this cell by placing New URLs in urls.txt file located in this folder.\n",
    "#The format of the New URL should be as follows:\n",
    "#URL,(1,-1)\n",
    "#i.e 1 = Legitimate , -1 = Phishing\n",
    "#Example : http://www.plu.sh/c3p7g/,-1\n",
    "#The New Dataset created is saved as new_dataset.csv under the local directory.\n",
    "#We can go on adding new URLs to the urls.txt file and dataset will be appended to new_dataset.csv\n",
    "\n",
    "import pandas as pd\n",
    "import feature_extraction as fe\n",
    "\n",
    "urls = open(\"urls.txt\", 'r')\n",
    "new_dataset = open(\"new_dataset.csv\", 'a')\n",
    "\n",
    "features = []\n",
    "\n",
    "for url in urls.readlines():\n",
    "    \n",
    "    label = int(url.strip().split(',')[1])\n",
    "    feat = fe.generate_data_set(url.split(',')[0])\n",
    "    feat += [label]\n",
    "    print(str(feat))\n",
    "    f = str(feat)[1:-1]\n",
    "    new_dataset.write(f)\n",
    "    new_dataset.write(\"\\n\")\n",
    "    #features.append(feat)\n",
    "new_dataset.close()\n",
    "urls.close()\n",
    "#features_df = pd.DataFrame(features)\n",
    "#features_df.to_csv(\"new_dataset.csv\")\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors  --  95.56761646313885 %\n",
      "RBF SVM  --  94.70827679782904 %\n",
      "Decision Tree  --  92.22071460877432 %\n",
      "Random Forest  --  89.64269561284488 %\n"
     ]
    }
   ],
   "source": [
    "#This Cell help us to generate a model and save it in pickle format.\n",
    "#Pickle is the standard way of serializing objects in Python.\n",
    "#The pickle operation is used to serialize the machine learning algorithms, later we can load the file to deserialize the model and use it to make new predictions.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "save_model_folder = \"model/\"\n",
    "directory = os.path.dirname(save_model_folder)\n",
    "if not os.path.exists(directory):\n",
    "\tos.makedirs(directory)\n",
    "    \n",
    "input_file = \"dataset.csv\"\n",
    " #Importing dataset\n",
    "data = np.loadtxt(\"dataset.csv\", delimiter = \",\")\n",
    "#Seperating features and labels\n",
    "X = data[: , :-1]\n",
    "y = data[: , -1]\n",
    "#Seperating training features, testing features, training labels & testing labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "h = .02  # step size in the mesh\n",
    "names = [\n",
    "\t\t\"Nearest Neighbors\",\n",
    "\t\t\"RBF SVM\",\n",
    "        \"Decision Tree\",\n",
    "\t\t\"Random Forest\"\n",
    "\t\t]\n",
    "classifiers = [\n",
    "           KNeighborsClassifier(3),\n",
    "           SVC(kernel='rbf', C=1, gamma='auto'),\n",
    "           DecisionTreeClassifier(max_depth=5),\n",
    "           RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "          ]\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    pickle.dump(clf, open(save_model_folder+name+\".sav\", 'wb'))\n",
    "    print(name,\" -- \",100*score,\"%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shorturl.at/fiF37\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 0, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1]\n",
      "Phishing Url\n"
     ]
    }
   ],
   "source": [
    "#In this cell we should specify a particular model which has obtained the highest accuracy during training/testing phase.\n",
    "#Then when we run the cell the user needs to input the URL in the text field.\n",
    "#According to the model trained with the dataset the system will try to predict that the given URL is Phishing or Legitimate\n",
    "#For example of Phishing website one can look into spam mailbox his/her email client.\n",
    "import numpy as np\n",
    "import feature_extraction\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from flask import jsonify\n",
    "import pickle\n",
    "\n",
    "def getResult(url):\n",
    "    clf = pickle.load(open(\"model/Nearest Neighbors.sav\", 'rb'))\n",
    "    X_new = []\n",
    "    X_input = url\n",
    "    X_new=feature_extraction.generate_data_set(X_input)\n",
    "    X_new = np.array(X_new).reshape(1,-1)\n",
    "\n",
    "    try:\n",
    "        prediction = clf.predict(X_new)\n",
    "        if prediction == -1:\n",
    "            return \"Phishing Url\"\n",
    "        else:\n",
    "            return \"Legitimate Url\"\n",
    "    except:\n",
    "        return \"Phishing Url\"\n",
    "\n",
    "URL = input()\n",
    "print(getResult(URL))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
